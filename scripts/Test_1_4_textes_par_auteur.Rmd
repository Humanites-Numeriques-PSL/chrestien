---
title: "Philologie computationnelle - TP 01: méthodes exploratoires"
author: "JBC & FC"
date: "14/10/2020"
output:
  pdf_document: default
  word_document: default
  html_document: default
---

# Setting up everything

## Choosing working folder

To begin with, you need to specify to `R` the current working directory (the folder you are working in, where your session will be saved, and where the files to be imported are located, etc.).

In `Rstudio`, you can do this by clicking on `session/set working directory/to source file location`.

Alternatively, you can specify the address with the `setwd()` command (set working directory). The address will depend on where the folder is located on your file system tree, and on your operating system (Unix or non-Unix). On the author's machine, this gives (commented out):

```{r}
# setwd("~/Data/Cours/Cours-Venise-2022/Cours/Computational_Philology/TP_Stylo_01_Navigating")
# and check
getwd()
```

Now, let's load some functions

```{r}
source("functions.R")
# install missing packages if needed
# install.packages("kohonen")
```


# Data 

## Load and clean texts

They are plenty of ways in R to load a corpus, clean it and create a term/document matrix. 

You can find such options in packages like:

- `stylo` (_stylometry_)
- `tidyverse`
- `tm` (_text mining_)

With `tm`,

```{r}
library("tm")
corpus = VCorpus(DirSource("corpus/test1", encoding = "UTF-8"), readerControl = list(language = "fra"))
#corpus = VCorpus(DirSource("corpus/02_Epubs/dumaquet/", encoding = "UTF-8"), readerControl = list(language = "fre"))
```

And then we can inspect it,
```{r}
inspect(corpus)
meta(corpus, "id")
```

## Cleaning

```{r}
corpus = tm_map(corpus, content_transformer(tolower))
# strip whitespace
corpus = tm_map(corpus, stripWhitespace)

inspect(DocumentTermMatrix(corpus))

myDTM = as.matrix(DocumentTermMatrix(corpus, control=list(removePunctuation=FALSE, 
                                       wordLengths=c(1, 50), 
                                       stopwords = FALSE,
                                       stemming = FALSE
                                       )))
```

Is our corpus balanced ?

```{r}
rowSums(myDTM)
summary(rowSums(myDTM))
barplot(rowSums(myDTM))
boxplot(rowSums(myDTM))
```
Not exactly… 
```{r}
folder_path <- "corpus/corpus/test1"
file_names <- list.files(folder_path, full.names = TRUE)


for (file in file_names) {
  text <- readLines(file, warn = FALSE)
  final_token = NA
  for (line in text) {
  token <- unlist(strsplit(line, " "))
  final_token <- append(final_token, token)
}
  target_length <- 1000
  start_index <- sample(1:(length(final_token) - target_length + 1), 1)
  final_text = paste(final_token[start_index:(start_index + target_length - 1)],sep = " ")
  writeLines(final_text, file)
}
```
## Sampling

So it would make sense from there to sample the documents, in order to get same size samples for each text.

The folder `corpus/prosa_samples` contains such random samples, of 1000 words, 9 by author

```{r}
samples = VCorpus(DirSource("corpus/test1", encoding = "UTF-8"), readerControl = list(language = "fra"))

samples = tm_map(samples, stripWhitespace)
samples = tm_map(samples, content_transformer(tolower))
inspect(DocumentTermMatrix(samples))
myDTM = as.matrix(DocumentTermMatrix(samples, control=list(removePunctuation=FALSE, 
                                       wordLengths=c(1, Inf), 
                                       stopwords = FALSE,
                                       stemming = FALSE
                                       )))
```

Is our corpus balanced now ?

```{r}
rowSums(myDTM)
summary(rowSums(myDTM))
barplot(rowSums(myDTM))
boxplot(rowSums(myDTM))
hist(rowSums(myDTM))
```

## Features extraction, culling and selection

### Words or n-grams ?

Now, we can proceed to choose words, function words or n-grams

```{r}
## Choose your side
## Words
data = t(myDTM)
## Affixes (from n-grams)
# data = countAffixes(t(myDTM))
```

Now, we will use survey method to keep only reliable features,

```{r}
select = selection(data, z = 1.645)
select = select[,4]
# Get relative frequencies
data_abs_save = data
data = relativeFreqs(data)
# keep only reliable features
data = data[select,]
```

Words users: Should we stick to function words only? 

In that case,
```{r}
function_words = scan("function_words_french.txt", what="character")
data = data[rownames(data) %in% function_words,]
```

# Dimensionality reduction

## Principal component analysis (PCA)

Let's use `FactoMineR`.

```{r, fig.width=10, fig.height=10, dpi=45}
# Load FactoMineR and factoextra for better visuals
library('FactoMineR')
library('factoextra')
# And compute ACP. As the function expects individuals as lines and variables as columns, I'm transposing it
monACP = PCA(t(data))
```

I can now look at the significance of my axes.

```{r}
monACP$eig
```
or plot it
```{r}
barplot(monACP$eig[,1], main="Eigenvalues", names.arg=1:nrow(monACP$eig))
```
Contribution:
```{r}
sort(monACP$var$contrib[,1], decreasing = TRUE)
```


Questions*: 
What can be concluded from this examination of the significance of the axes? Which axes deserve to be examined closely? 

Let us now examine the first factorial design (axes 1-2)

```{r, fig.width=10, fig.height=10, dpi=45}
plot.PCA(monACP, axes = c(3,4))
# And with slightly better looking plot
fviz_pca_ind(monACP, col.ind = sub("\\_.*", "", labels(monACP$ind$dist)), title="", geom=c("point"))
fviz_pca_ind(monACP, col.ind = sub("\\_.*", "", labels(monACP$ind$dist)), title="", geom=c("point", "text"), labelsize=2)
fviz_pca_var(monACP, title="", geom=c("text"), labelsize=2)
```

*Question: Is this result relevant? How can it be interpreted? 


## Correspondance analysis (CA)


```{r, fig.width=10, fig.height=10, dpi=45}

library("FactoMineR")
myCA = CA(data, ncp = 5, row.sup = NULL, col.sup = NULL, 
    quanti.sup=NULL, quali.sup = NULL, graph = TRUE, 
	axes = c(1,2), row.w = NULL, excl=NULL)


fviz_ca_row(myCA, geom=c("text"), labelsize=2)

fviz_ca_col(myCA, col.col = sub("\\_.*", "", rownames(myCA$col$coord)), title="", geom=c("point"), labelsize=2)
fviz_ca_col(myCA, col.col = sub("\\_.*", "", rownames(myCA$col$coord)), title="", geom=c("point", "text"), labelsize=2)
```


## T-SNE

```{r, fig.width=10, fig.height=10, dpi=45}
library(Rtsne)
maRtsne = Rtsne(t(data), dims = 2, initial_dims = 36, perplexity = 1, theta = 0.0, check_duplicates = TRUE, pca = TRUE)
plot(maRtsne$Y)
text(maRtsne$Y[,1], maRtsne$Y[,2], labels = row.names(t(data)), cex=.6) 
#Variation de perplexité
maRtsne = Rtsne(t(data), dims = 2, initial_dims = 36, perplexity = 3,  theta = 0.0, check_duplicates = TRUE, pca = TRUE)
plot(maRtsne$Y)
text(maRtsne$Y[,1], maRtsne$Y[,2], labels = row.names(t(data)), cex=.6) 
#Perplexity = 5
maRtsne = Rtsne(t(data), dims = 2, initial_dims = 36, perplexity = 5,  theta = 0.0, check_duplicates = TRUE, pca = TRUE)
plot(maRtsne$Y)
text(maRtsne$Y[,1], maRtsne$Y[,2], labels = row.names(t(data)), cex=.6) 
```

## Multi-dimensional scaling

### Classic / metric

Now, instead of cutting dimensions, we are smashing them.

```{r}
#Euclid
fit = cmdscale(dist(t(data), method = "euclid"), eig=TRUE, k=2) # k is the number of expected dims
```

Significativity of dimensions (_eigenvalues_):
```{r}
barplot(fit$eig,main="Eigenvalues", names.arg=1:length(fit$eig))
```

Traçons à présent le graphique,
```{r, fig.width=10, fig.height=10, dpi=45}
x = fit$points[,1]
y = fit$points[,2]
plot(x, y, xlab=paste("Coordinate 1 (GOF: ", round(fit$GOF[1] * 100, digits=2), "%)"), ylab=paste("Coordinate 2 (GOF: ", round(fit$GOF[2] * 100, digits=2), "%)"), main="PMD métrique")
text(x, y, labels = row.names(t(data)), cex=.7) 
```

### Metric

The metric MDS is a generalisation that allows the use of other distance calculations, weights, etc.

```{r, fig.width=10, fig.height=10, dpi=45}
library("smacof")
monMds = mds(dist(t(data), method = "manhattan"), ndim=2, type="interval")
plot(monMds, sub=paste("Stress, ", round(monMds$stress, digits=2)))
```

### Non metric


```{r, fig.width=10, fig.height=10, dpi=45}
library(MASS)
fit = isoMDS(dist(t(data), method = "manhattan"), k=2)#Ou option ordinal de smacof::mds
#fit = isoMDS(MinMax(theatreTraite), k=2)
fit
```

and plot

```{r, fig.width=10, fig.height=10, dpi=45}
x = fit$points[,1]
y = fit$points[,2]
plot(x, y, xlab="Coord 1", ylab="Coord 1",
  main="non metric MDS", type="n", sub=paste("Stress, ", round(fit$stress, digits=2)))
text(x, y, labels = row.names(t(data)), cex=.7) 
```


# Clustering

## Selecting features

## Weighting

In oppositions to analyses like Correspondance analysis, that perform their own kind of 'centering' of the variables, here, we will do two types of normalisations:

- variables (rows) with z-scores;
- individuals (columns) with vector length normalisation (Euclidean norm).

```{r}
data = normalisations(data)
```

And let's go.

## Clustering methods: hierarchical clustering


```{r, fig.width=20, fig.height=10, dpi=45}
library(cluster)
maCAH = agnes(t(data), metric = "manhattan", method="ward")
plot(maCAH, which.plots = 2)
# Et une fonction un peu plus esthétique
maCAH_save = maCAH
#cahPlotCol(maCAH, k = 5)
```
## Distance metrics and agglomeration method

### Distance metrics

The choice of a distance measure can have a considerable effect on the results. We can use several.

Among the metrics that are given, both by Jannidis et al. 2015 and Kestemont et al. 2016, as among the most effective, we will note the cosine distance,

```{r, fig.width=20, fig.height=10, dpi=45}
library(stylo)
maCAH = agnes(dist.cosine(t(data)), method="ward")
plot(maCAH, which.plots = 2)
#cahPlotCol(maCAH, k = 9)
```

A more recent metric with interesting performance is also the MinMax metric (Koppel & Winter 2014). As it is not implemented, we will create a function for it.

```{r}
MinMax =
function(x){
  myDist = matrix(nrow = ncol(x),ncol = ncol(x), dimnames = list(colnames(x),colnames(x)))
  for(i in 1:ncol(x)){
    for(j in 1:ncol(x)){
      min = sum(apply(cbind(x[,i],x[,j]), 1, min))
      max = sum(apply(cbind(x[,i],x[,j]), 1, max))
      resultat = 1 - (min / max)
      myDist[i,j] = resultat
    }
  }
  return(myDist)
}

```

```{r, fig.width=20, fig.height=10, dpi=45}
maCAH = agnes(MinMax(data), method="ward")
plot(maCAH, which.plots = 2)
#cahPlotCol(maCAH, k = 9)
```

## Heatmaps

An interesting visualization to complement the CAH view is the heat map ( _heatmap_ ), where proximities are indicated by a colour gradient.

```{r, fig.width=20, fig.height=10, dpi=45}
maCAH2 = as.dendrogram(maCAH_save)
heatmap(as.matrix(dist(t(data), method = "manhattan")), Rowv = maCAH2, Colv = maCAH2, symm=TRUE, margins = c(12,12), scale = 'row')
#heatmap(data)
```

<!-- Refaire avec ggplot2 - pour avoir height et AC, ainsi que l'échelle des couleurs -->


## K-Medoids

Let's try to make 3 groups in our data

```{r}
maPAM = cluster::pam(t(data), 3, metric = "manhattan")
maPAM$clustering
maPAM$silinfo
```

## DBScan

```{r}
#install.packages("fpc")
#install.packages("dbscan")
#install.packages("factoextra")
# http://www.sthda.com/english/wiki/wiki.php?id_contents=7940
#dbscan(data, eps, MinPts = 5, scale = FALSE, 
#       method = c("hybrid", "raw", "dist"))
monSCAN = fpc::dbscan(dist(t(data), method="manhattan"), method="dist", eps = 12, MinPts = 2, showplot=TRUE)
# Visualiser les résultats par classe
cbind(colnames(data), monSCAN$cluster)
# Plot
plot(monSCAN, data, main = "DBSCAN", frame = FALSE)
```

# Class description


### Creating groups: cutting an AHC

If we want to describe a classification, in terms of the specificities of each group, several solutions are possible. We can start from supervised clustering from K-means or K-medoids, or from the cut of a CAH.

The number of classes should be chosen according to the dendrogram, and a height graph can be used (corresponding to the heights at which the branches of the dendrogram are separated).


```{r}
maCAH = maCAH_save
maCAH2 = as.hclust(maCAH)
plot(maCAH2$height, type="h", ylab="heights")
#cahPlotCol(maCAH, k = 4)
```


And let's cut

```{r}
#Je découpe en classes
classes = cutree(maCAH, k = "4") # ="4"
#J'ajoute les classes à mon tableau
myClassif = t(data)
myClassif = cbind(as.data.frame(myClassif), as.factor(classes))
colnames(myClassif[length(myClassif)])[] = "Classes"
```


```{r}
myClassif[length(myClassif)]
```

### Describing classes

Let's try to understand now what put _Istorietta_ and _Novella_ in the same class.


Let's do a v-test,

```{r, fig.width=20, fig.height=10, dpi=45}
#Je charge FactoMineR
library(FactoMineR)
#Et je décrit les classes
mesClasses = catdes(myClassif, num.var = length(myClassif))
# $\eta^2$
mesClasses$quanti.var[1:20,]
# v-test pour la classe 1
mesClasses$quanti$`2`
plot(mesClasses, level = 0.01, barplot = TRUE)
```

```{r}
classesDesc(maCAH, data, k = 4)
```

It is also possible to use other computations of specificities, like Lafon (1984), that needs absolute frequencies.

```{r}
# install.packages("textometry")
freq_abs = data_abs_save[rownames(data_abs_save) %in% function_words,]
# On splitte le corpus selon les classes
freq_abs_class = matrix(nrow = nrow(freq_abs), ncol = length(unique(classes)), dimnames = list(rownames(freq_abs), unique(classes)))
for(i in 1:length(unique(classes))){
  # sum the values for each member of the class
  freq_abs_class[, i] = rowSums(freq_abs[, classes == i])
}
specs = textometry::specificities(freq_abs_class)
# and now we can look at the specificities for class 1 
# positive or negative
head(sort(specs[, 2], decreasing = TRUE))
head(sort(specs[, 2]))
```

And one can use other varieties of non-parametric tests, such as the Mann-Whitney test, for example, to compare two populations...

# Bonus: Networks

Cf. Maciej Eder, Visualization in stylometry: Cluster analysis using networks, Digital Scholarship in the Humanities, Volume 32, Issue 1, April 2017, Pages 50–64, https://doi.org/10.1093/llc/fqv061.


# Appendix 

## Install or update R packages

One of the great strengths of `R' is the large community that surrounds it, and that enriches the language by creating additional modules (_packages_) dedicated to this or that type of analysis. In order to use them, it is necessary to install them, and, when needed, to load them.

```{r}
# # installing packages
# install.packages('stylo', dependencies=TRUE)
# install.packages('FactoMineR', dependencies=TRUE)
# # and to load
# library(stylo)
# # and to update
#update.packages()
#install.packages("kohonen")
```
